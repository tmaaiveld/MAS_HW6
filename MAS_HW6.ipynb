{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "deadline: <font color='red'>6 Jan</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 $\\enspace$ Multi-Armed Bandits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Thompson Sampling for single bandit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a bandit that for each pull of the arm, produces a binary reward: $r = 1$ (with probability $p$)\n",
    "or $r = 0$ (with probability $1-p$). We model our uncertainty about the actual (but unknown) value\n",
    "$p$ using a beta-distribution (cf. https://en.wikipedia.org/wiki/Beta_distribution). This\n",
    "is a probability distribution on the interval [0; 1] which depends on two parameters: $\\alpha, \\beta \\geq 1$.\n",
    "The explicit distribution is given by (for $\\alpha, \\beta$ integers!):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "B(x;\\alpha,\\beta) = \\frac{(\\alpha + \\beta - 1)!}{(\\alpha - 1)!(\\beta - 1)!}x^{\\alpha - 1}(1-x)^{\\beta - 1} \\ \\ \\ \\ (\\text{for} \\ 0 \\leq x \\leq 1)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters $\\alpha$ and $\\beta$ determine the shape of the distribution:\n",
    "- If $\\alpha = \\beta = 1$ then we have the uniform distribution;\n",
    "- If $\\alpha = \\beta$ the distribution is symmetric about $x = \\frac{1}{2}$.\n",
    "- If $\\alpha > \\beta$ the density is right-leaning (i.e. concentrated in the neighbourhood of 1). In fact, one can compute the mean explicitly:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation} \n",
    "X \\sim B(x;\\alpha,\\beta) \\enspace \\implies \\enspace \\mathbb{E}[X] = \\frac{\\alpha}{\\alpha + \\beta}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Larger values of $\\alpha$ and $\\beta$ produce a more peaked distribution. This follows from the formula\n",
    "for the variance:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "X \\sim B(x;\\alpha,\\beta) \\enspace \\implies \\enspace Var(X) = \\frac{\\alpha\\beta}{(\\alpha+\\beta)^2(\\alpha + \\beta + 1)}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Thompson update rule}$ Assume that we don't know the success probability $p$ for the bandit.\n",
    "The Thompson update rule for a single bandit proceeds as follows:\n",
    "- Initialise $\\alpha = \\beta = 1$ (resulting in a uniform distribution, indicating that all values for $p$ are equally likely). Now repeat the following loop:\n",
    "    1. Sample from the bandit and get reward $r$ (either 1 or 0);\n",
    "    2. Update the values for $\\alpha$ and $\\beta$ as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\alpha \\leftarrow \\alpha + r \\qquad \\beta \\leftarrow \\beta + (1 - r)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Questions}$\n",
    "- Make several plots of the Beta-density to illustrate the properties (dependence on the parameters) outlined above;\n",
    "- Implement the Thompson update rule and show experimentally that the Beta-density increasingly peaks at the correct value for $p$. Plot both the evolution and the mean and variance over (iteration) time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Thompson Sampling}$ Suppose we have a 2-bandit problem. The first one delivers a reward $r = 1$ with (unknown!) probability $p_1$, while the second one does so with (unknown!) probability $p_2$. For each bandit ($k = 1,2$), the uncertainty about the corresponding $p_k$ is modelled using a Beta-distribution with coefficients $\\alpha_k, \\beta_k$. Thompson sampling now tries to identify the bandit that delivers the maximal output and proceeds as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Initialise all parameters to 1: $\\alpha_k = 1 = \\beta_k$; \n",
    "Now repeat the following loop:\n",
    "1. Sample the value $u_k$ for each of the two Beta-distributions: \n",
    "\n",
    "\\begin{equation}\n",
    "u_k \\sim B(x;\\alpha_k,\\beta_k)\n",
    "\\end{equation}\n",
    "\n",
    "2. Determine the max: $k_m = \\text{argmax}\\{u_1,u_2\\}$\n",
    "3. Sample the corresponding bandit and get reward $r$ (either 1 or 0);\n",
    "4. Update the corresponding parameters: $\\alpha_{k_m}$ and $\\beta_{k_m}$ as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "\\alpha_{k_m} \\rightarrow \\alpha_{k_m} + r \\quad\n",
    "\\enspace \\text{and} \\enspace \n",
    "\\quad \\beta_{k_m} \\rightarrow \\beta_{k_m} + (1 - r)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Questions}$\n",
    "- Write code to implement Thompson sampling for the above scenario;\n",
    "- Perform numerical experiments in which you compare Thompson sampling with the UCB and $\\epsilon$-greedy approach. Define some performance measures and discuss the performance of each algorithm with respect to these measures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 $\\enspace$ Reinforcement Learning: Cliff Walking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the cliff-walking example (Sutton & Barto, ex. 6.6 p.108). Assume that the grid has 10 columns and 5 rows (above or in addition to the cliff). This is a standard undiscounted, episodic task, with start and goal states, and the usual actions causing movement up, down, right and left. Reward is $-1$ on all transitions except:\n",
    "- the transition to the terminal goal state (G) which has an associated reward of $+20$;\n",
    "- transitions into the region marked \\textit{The Cliff}. Stepping into this region incurs a \"reward\" of $-100$ and also terminates the episode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/cliff_example.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Questions}$\n",
    "1. Use both SARSA and Q-Learning to construct an appropriate policy. Do you observe the difference between the SARSA and Q-learning policies mentioned in the text (safe versus optimal path)? Discuss.\n",
    "2. Try different values for $\\epsilon$ (parameter for $\\epsilon$-greedy policy). How does the value of $\\epsilon$ influence the result? Discuss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
